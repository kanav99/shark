# -*- coding: utf-8 -*-
"""CIFAR10 using 3 layer CNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KsqBrCOHQjCe9hb84rJsXNcly_8RCghY
"""

import torch
import numpy as np
from torchvision import datasets
import torchvision.transforms as transforms
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data.sampler import SubsetRandomSampler
from torch.autograd import Variable
import torchsummary

# check if CUDA is available
train_on_gpu = torch.cuda.is_available()

if not train_on_gpu:
    print('CUDA is not available.  Training on CPU ...')
else:
    print('CUDA is available!  Training on GPU ...')

train_dataset = datasets.CIFAR10(root='data', train=True, transform=transforms.ToTensor(), download=True)
test_dataset = datasets.CIFAR10(root='data', train=False, transform=transforms.ToTensor())

batch_size = 100
lr_rate = 1.0/64.0
momentum = 58.0 / 64.0
# momentum = 0.0
epochs = 10

train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False)
test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)

class Net(nn.Module):
  def __init__(self):
    super(Net, self).__init__()
    self.pool1 = nn.MaxPool2d(3, 2)
    self.pool2 = nn.MaxPool2d(3, 2)
    self.pool3 = nn.MaxPool2d(3, 2)
    self.conv1 = nn.Conv2d(3,  64, 5, 1, 1)
    # self.bn1 = nn.BatchNorm2d(64)
    # self.bn2 = nn.BatchNorm2d(64)
    # self.bn3 = nn.BatchNorm2d(64)
    self.conv2 = nn.Conv2d(64, 64, 5, 1, 1)
    self.conv3 = nn.Conv2d(64, 64, 5, 1, 1)
    self.fc1 = nn.Linear(64, 10)

  def forward(self, x):
    # x = self.pool(F.relu(self.bn1(self.conv1(x))))
    # x = self.pool(F.relu(self.bn2(self.conv2(x))))
    # x = self.pool(F.relu(self.bn3(self.conv3(x))))
    x = self.pool1(F.relu((self.conv1(x))))
    x = self.pool2(F.relu((self.conv2(x))))
    x = self.pool3(F.relu((self.conv3(x))))
    # x = x.view(-1, 64)
    x = x.flatten(start_dim=1)
    x = self.fc1(x)
    # x = F.softmax(x)
    return x

model = Net()
# model.conv1.
# print(model.conv1.weight.shape)
from torchsummary import summary
if train_on_gpu:
  model.cuda()
summary(model, (3, 32, 32))

import torch.optim as optim
# specify loss function
criterion = nn.CrossEntropyLoss()
# specify optimizer
optimizer = optim.SGD(model.parameters(), lr=lr_rate, momentum=momentum)

iter = 0
accuracies = []
for epoch in range(int(epochs)):
    model.train()
    for i, (images, labels) in enumerate(train_loader):
        # labels = Variable(labels)
        if train_on_gpu:
            images, labels = images.cuda(), labels.cuda()
        optimizer.zero_grad()
        # print(F.one_hot(labels, num_classes=10))
        outputs = model(images)
        # loss = criterion(outputs, F.one_hot(labels, num_classes=10).float())
        loss = criterion(outputs, labels)
        # print(loss)
        loss.backward()
        # print(loss)
        optimizer.step()
        iter+=1
    correct = 0
    total = 0
    model.eval()
    for images, labels in test_loader:
        if train_on_gpu:
            images, labels = images.cuda(), labels.cuda()
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total+= labels.size(0)
        # for gpu, bring the predicted and labels back to cpu fro python operations to work
        correct+= (predicted == labels).sum()
    accuracy = 100 * correct/total
    accuracies += [accuracy]
    print("Epoch: {}. Accuracy: {}.".format(epoch, accuracy))

torch.save(model.state_dict(), 'weights.pt')
